This file lists all the possible signatures of functions/functors/lambdas that can be used to
create WindFlow operators. In the signatures below, tuple_t is the type of the inputs arriving
at an operator, while result_t is the type of the results.

SOURCE
------
bool(tuple_t &)
bool(tuple_t &, RuntimeContext &)
bool(Shipper<tuple_t> &)
bool(Shipper<tuple_t> &, RuntimeContext &)

FILTER
------
bool(tuple_t &)
bool(tuple_t &, RuntimeContext &)
std::optional<result_t>(const tuple_t &)
std::optional<result_t>(const tuple_t &, RuntimeContext &)
std::optional<result_t *>(const tuple_t &)
std::optional<result_t *>(const tuple_t &, RuntimeContext &)

MAP
---
void(tuple_t &)
void(tuple_t &, RuntimeContext &)
void(const tuple_t &, result_t &)
void(const tuple_t &, result_t &, RuntimeContext &)

FLATMAP
-------
void(const tuple_t &, Shipper<result_t> &)
void(const tuple_t &, Shipper<result_t> &, RuntimeContext &)

ACCUMULATOR
-----------
void(const tuple_t &, result_t &)
void(const tuple_t &, result_t &, RuntimeContext &)

KEY_FARM
--------
void(uint64_t, const Iterable<tuple_t> &, result_t &)
void(uint64_t, const Iterable<tuple_t> &, result_t &, RuntimeContext &)
void(uint64_t, const tuple_t &, result_t &)
void(uint64_t, const tuple_t &, result_t &, RuntimeContext &)

KEY_FFAT
--------
This operator needs two functions (lift and combine) with the following accepted signatures

Lift
	void(const tuple_t &, result_t &)
    void(const tuple_t &, result_t &, RuntimeContext &)

Combine
    void(const result_t &, const result_t &, result_t &)
    void(const result_t &, const result_t &, result_t &, RuntimeContext &)

WIN_FARM
--------
void(uint64_t, const Iterable<tuple_t> &, result_t &)
void(uint64_t, const Iterable<tuple_t> &, result_t &, RuntimeContext &)
void(uint64_t, const tuple_t &, result_t &)
void(uint64_t, const tuple_t &, result_t &, RuntimeContext &)

PANE_FARM
---------
This operator needs two functions (PLQ and WLQ) with the following accepted signatures

PLQ
	void(uint64_t, const Iterable<tuple_t> &, result_t &)
	void(uint64_t, const Iterable<tuple_t> &, result_t &, RuntimeContext &)
	void(uint64_t, const tuple_t &, result_t &)
	void(uint64_t, const tuple_t &, result_t &, RuntimeContext &)

WLQ
	void(uint64_t, const Iterable<result_t> &, result_t &)
	void(uint64_t, const Iterable<result_t> &, result_t &, RuntimeContext &)
	void(uint64_t, const result_t &, result_t &)
	void(uint64_t, const result_t &, result_t &, RuntimeContext &)

WIN_MAPREDUCE
-------------
This operator needs two functions (MAP and REDUCE) with the following accepted signatures

MAP
	void(uint64_t, const Iterable<tuple_t> &, result_t &)
	void(uint64_t, const Iterable<tuple_t> &, result_t &, RuntimeContext &)
	void(uint64_t, const tuple_t &, result_t &)
	void(uint64_t, const tuple_t &, result_t &, RuntimeContext &)

REDUCE
	void(uint64_t, const Iterable<result_t> &, result_t &)
	void(uint64_t, const Iterable<result_t> &, result_t &, RuntimeContext &)
	void(uint64_t, const result_t &, result_t &)
	void(uint64_t, const result_t &, result_t &, RuntimeContext &)

KEY_FARM_GPU
------------
__host__ __device__ (uint64_t, const tuple_t *, size_t, result_t *, char *, size_t)

KEY_FFAT_GPU
------------
This operator needs two functions (lift and combine) with the following accepted signatures. Only the combine function must be __host__ __device__

Lift
	void(const tuple_t &, result_t &)

Combine
	__host__ __device__ (const result_t &, const result_t &, result_t &)

WIN_FARM_GPU
------------
__host__ __device__ (uint64_t, const tuple_t *, size_t, result_t *, char *, size_t)

PANE_FARM_GPU
-------------
This operator needs two functions (PLQ and WLQ) with the following accepted signatures

PLQ
	void(uint64_t, const Iterable<tuple_t> &, result_t &) --> PLQ on Host
	void(uint64_t, const tuple_t &, result_t &) --> PLQ on Host
	__host__ __device__ (uint64_t, const tuple_t *, size_t, result_t *, char *, size_t) --> PLQ on Device

WLQ
	void(uint64_t, const Iterable<result_t> &, result_t &) --> WLQ on Host
	void(uint64_t, const Iterable<result_t> &, result_t &, RuntimeContext &) --> WLQ on Host
	__host__ __device__ (uint64_t, const result_t *, size_t, result_t *, char *, size_t) --> WLQ on Device

Important -> either the PLQ or the WLQ must be __host__ __device__ functions, not both!

WIN_MAPREDUCE_GPU
-----------------
This operator needs two functions (MAP and REDUCE) with the following accepted signatures

MAP
	void(uint64_t, const Iterable<tuple_t> &, result_t &) --> MAP on Host
	void(uint64_t, const tuple_t &, result_t &) --> MAP on Host
	__host__ __device__ (uint64_t, const tuple_t *, size_t, result_t *, char *, size_t) --> MAP on Device

REDUCE
	void(uint64_t, const Iterable<result_t> &, result_t &) --> REDUCE on Host
	void(uint64_t, const Iterable<result_t> &, result_t &, RuntimeContext &) --> REDUCE on Host
	__host__ __device__ (uint64_t, const result_t *, size_t, result_t *, char *, size_t) --> REDUCE on Device

Important -> either the MAP or the REDUCE must be __host__ __device__ functions, not both!

SINK
----
void(std::optional<tuple_t> &)
void(std::optional<tuple_t> &, RuntimeContext &)
void(std::optional<std::reference_wrapper<tuple_t>>)
void(std::optional<std::reference_wrapper<tuple_t>>, RuntimeContext &)

SPLITTING OF MULTIPIPES
-----------------------
size_t(const tuple_t &) --> each input delivered to one MultiPipe only (unicast)
std::vector<size_t>(const tuple_t &) --> each input delivered to one/many/all destination MultiPipes (multicast/broadcast)
